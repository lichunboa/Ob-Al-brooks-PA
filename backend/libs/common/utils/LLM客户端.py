#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""LLM APIå®¢æˆ·ç«¯å·¥å…·

ä¸ºå…¶ä»–æœåŠ¡æä¾›ç®€å•çš„æ¥å£æ¥è°ƒç”¨LLM APIç½‘å…³ï¼Œè‡ªåŠ¨å¤„ç†èº«ä»½è®¤è¯å’Œè¯·æ±‚ã€‚
"""

import json
import requests
from typing import Dict, List, Optional, Any
from libs.common.utils.è·¯å¾„åŠ©æ‰‹ import è·å–ä»“åº“æ ¹ç›®å½•


class LLMå®¢æˆ·ç«¯:
    """LLM APIç½‘å…³å®¢æˆ·ç«¯

    æä¾›ç®€å•çš„æ¥å£è°ƒç”¨LLM APIæœåŠ¡ï¼Œè‡ªåŠ¨ä».envæ–‡ä»¶è¯»å–é…ç½®ã€‚

    ç¤ºä¾‹:
        client = LLMå®¢æˆ·ç«¯()
        response = client.èŠå¤©(
            messages=[{"role": "user", "content": "Hello!"}],
            model="gemini-2.5-flash"
        )
        print(response["choices"][0]["message"]["content"])
    """

    def __init__(self, base_url: Optional[str] = None, api_key: Optional[str] = None):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯

        Args:
            base_url: LLM APIç½‘å…³åœ°å€ï¼Œé»˜è®¤ä».envè¯»å–æˆ–ä½¿ç”¨localhost:8000
            api_key: å¤–éƒ¨è®¿é—®å¯†é’¥ï¼Œé»˜è®¤ä».envè¯»å–
        """
        from dotenv import load_dotenv
        import os

        # åŠ è½½.envæ–‡ä»¶ï¼ˆç»Ÿä¸€è¯»å– config/.envï¼‰
        env_path = è·å–ä»“åº“æ ¹ç›®å½•() / "config" / ".env"
        load_dotenv(env_path)

        # è®¾ç½®APIåœ°å€
        self.base_url = base_url or os.getenv("LLM_API_BASE_URL", "http://localhost:8000")
        self.api_key = api_key or os.getenv("EXTERNAL_API_KEY")

        if not self.api_key:
            raise ValueError(
                f"æœªæ‰¾åˆ°EXTERNAL_API_KEYé…ç½®ï¼Œè¯·ç¡®ä¿åœ¨ {env_path} æ–‡ä»¶ä¸­è®¾ç½®äº†è¯¥å€¼"
            )

        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    def èŠå¤©(
        self,
        messages: List[Dict[str, str]],
        model: str = "gemini-2.5-flash",
        temperature: float = 0.7,
        max_tokens: int = 1000,
        stream: bool = False,
        req_timeout: int = 60,
        **kwargs
    ) -> Dict[str, Any]:
        """å‘é€èŠå¤©è¯·æ±‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼: [{"role": "user", "content": "..."}]
            model: æ¨¡å‹åç§°ï¼Œæ”¯æŒ gemini-2.5-flash, gemini-pro, gpt-3.5-turbo, gpt-4
            temperature: æ¸©åº¦å‚æ•°ï¼Œ0-2ä¹‹é—´
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å“åº”
            **kwargs: å…¶ä»–å‚æ•°ï¼Œå¦‚top_p, n, stopç­‰

        Returns:
            APIå“åº”å­—å…¸

        ç¤ºä¾‹:
            response = client.èŠå¤©(
                messages=[
                    {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
                ],
                model="gemini-2.5-flash",
                temperature=0.7
            )
            content = response["choices"][0]["message"]["content"]
        """
        url = f"{self.base_url}/v1/chat/completions"

        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream,
            **kwargs
        }

        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=req_timeout)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise Exception(f"LLM APIè¯·æ±‚å¤±è´¥: {str(e)}")

    def åµŒå…¥(self, input_text: str, model: str = "text-embedding-ada-002") -> Dict[str, Any]:
        """åˆ›å»ºæ–‡æœ¬åµŒå…¥å‘é‡

        Args:
            input_text: è¾“å…¥æ–‡æœ¬
            model: åµŒå…¥æ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨ text-embedding-ada-002

        Returns:
            åŒ…å«åµŒå…¥å‘é‡çš„å“åº”
        """
        url = f"{self.base_url}/v1/embeddings"

        payload = {
            "model": model,
            "input": input_text
        }

        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=60)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise Exception(f"åµŒå…¥APIè¯·æ±‚å¤±è´¥: {str(e)}")

    def è·å–æ¨¡å‹åˆ—è¡¨(self) -> List[Dict[str, Any]]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨

        Returns:
            æ¨¡å‹åˆ—è¡¨
        """
        url = f"{self.base_url}/v1/models"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            return response.json().get("data", [])
        except requests.exceptions.RequestException as e:
            raise Exception(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")

    def è·å–ç»Ÿè®¡ä¿¡æ¯(self) -> Dict[str, Any]:
        """è·å–APIä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«å¯†é’¥çŠ¶æ€ã€è¯·æ±‚ç»Ÿè®¡ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        url = f"{self.base_url}/stats"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise Exception(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

    def å¥åº·æ£€æŸ¥(self) -> bool:
        """æ£€æŸ¥LLM APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ

        Returns:
            Trueè¡¨ç¤ºæœåŠ¡æ­£å¸¸ï¼ŒFalseè¡¨ç¤ºæœåŠ¡å¼‚å¸¸
        """
        url = f"{self.base_url}/"

        try:
            response = requests.get(url, headers=self.headers, timeout=5)
            return response.status_code == 200
        except:
            return False


def åˆ›å»ºLLMå®¢æˆ·ç«¯() -> LLMå®¢æˆ·ç«¯:
    """å¿«é€Ÿåˆ›å»ºLLMå®¢æˆ·ç«¯å®ä¾‹

    ç¤ºä¾‹:
        client = åˆ›å»ºLLMå®¢æˆ·ç«¯()
        response = client.èŠå¤©([{"role": "user", "content": "Hello!"}])
        print(response)
    """
    return LLMå®¢æˆ·ç«¯()


# é¢„å®šä¹‰çš„ç³»ç»Ÿæç¤ºæ¨¡æ¿
ç³»ç»Ÿæç¤ºæ¨¡æ¿ = {
    "ä»£ç å®¡æŸ¥": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥åŠ©æ‰‹ã€‚è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç ï¼Œå¹¶æä¾›æ”¹è¿›å»ºè®®ï¼š",
    "æ–‡æ¡£ç”Ÿæˆ": "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£ç¼–å†™åŠ©æ‰‹ã€‚è¯·ä¸ºä»¥ä¸‹ä»£ç ç”Ÿæˆæ¸…æ™°çš„æ–‡æ¡£ï¼š",
    "é”™è¯¯åˆ†æ": "ä½ æ˜¯ä¸€ä¸ªé”™è¯¯åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹é”™è¯¯ä¿¡æ¯ï¼Œå¹¶æä¾›è§£å†³æ–¹æ¡ˆï¼š",
    "ä¼˜åŒ–å»ºè®®": "ä½ æ˜¯ä¸€ä¸ªä»£ç ä¼˜åŒ–ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹ä»£ç ï¼Œå¹¶æä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š",
}


if __name__ == "__main__":
    # ç®€å•çš„æµ‹è¯•
    print("ğŸ§ª æµ‹è¯•LLMå®¢æˆ·ç«¯...")

    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = åˆ›å»ºLLMå®¢æˆ·ç«¯()

        # æ‰§è¡Œå¥åº·æ£€æŸ¥
        if client.å¥åº·æ£€æŸ¥():
            print("âœ… LLM APIæœåŠ¡æ­£å¸¸è¿è¡Œ")
        else:
            print("âŒ LLM APIæœåŠ¡ä¸å¯ç”¨")
            exit(1)

        # è·å–æ¨¡å‹åˆ—è¡¨
        models = client.è·å–æ¨¡å‹åˆ—è¡¨()
        print(f"âœ… å¯ç”¨æ¨¡å‹æ•°é‡: {len(models)}")
        for model in models[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
            print(f"   - {model['id']} ({model['owned_by']})")

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = client.è·å–ç»Ÿè®¡ä¿¡æ¯()
        print(f"âœ… æ´»è·ƒå¯†é’¥æ•°: {stats['active_keys']}/{stats['total_keys']}")

        # æµ‹è¯•èŠå¤©ï¼ˆå¯é€‰ï¼‰
        # response = client.èŠå¤©(
        #     messages=[{"role": "user", "content": "Hello, are you working?"}],
        #     max_tokens=50
        # )
        # print(f"âœ… æµ‹è¯•å“åº”: {response['choices'][0]['message']['content'][:50]}...")

        print("\nğŸ‰ LLMå®¢æˆ·ç«¯æµ‹è¯•å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        print("\nè¯·ç¡®ä¿ï¼š")
        print("1. LLM APIæœåŠ¡å·²å¯åŠ¨ (python services/llm-service/src/api/llm_api.py)")
        print("2. æ ¹ç›®å½•çš„.envæ–‡ä»¶é…ç½®äº†æ­£ç¡®çš„EXTERNAL_API_KEY")
        print("3. ç½‘ç»œè¿æ¥æ­£å¸¸")
        exit(1)
